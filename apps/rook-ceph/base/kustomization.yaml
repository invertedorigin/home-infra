apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- namespace.yaml

namespace: rook-ceph

helmCharts:
- name: rook-ceph
  repo: https://charts.rook.io/release
  version: v1.15.4
  releaseName: rook-ceph
  valuesInLine:
    monitoring:
      enabled: true

- name: rook-ceph-cluster
  repo: https://charts.rook.io/release
  version: v1.15.4
  releaseName: rook-ceph-cluster
  namespace: rook-ceph
  valuesInLine:
    operatorNamespace: rook-ceph
    cephClusterSpec:
      network:
        hostNetwork: true
      mgr:
        modules:
          - name: pg_autoscaler # This is already enabled by default, but needs to be here so that we don't override the default behavior.
            enabled: true
          - name: rook # This enables the rook module.
            enabled: true
      resources:
        osd:
          limits:
            memory: "4Gi"
          requests:
            memory: "1Gi"
        mds:
          requests:
            memory: "1Gi"
          limits:
            memory: "4Gi"
      storage:
        useAllNodes: false
        useAllDevices: false
        nodes:
          - name: node-1
            config:
              osdsPerDevice: "1"
            devices:
              - name: "/dev/disk/by-id/nvme-WD_BLACK_SN850X_2000GB_24163X803861"
          # - name: node-2
          #   config:
          #     osdsPerDevice: "1"
          #   devices:
          #     - name: "/dev/disk/by-id/nvme-WD_BLACK_SN850X_2000GB_24163X803862-part1"
          #     - name: "/dev/disk/by-id/nvme-WD_BLACK_SN850X_2000GB_24163X803862-part2"
          - name: node-3
            config:
              osdsPerDevice: "1"
            devices:
              - name: "/dev/disk/by-id/nvme-WD_BLACK_SN850X_2000GB_24163X803932-part1"
              - name: "/dev/disk/by-id/nvme-WD_BLACK_SN850X_2000GB_24163X803932-part2"
    cephFileSystems:
      - name: ceph-filesystem
        # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
        spec:
          metadataPool:
            replicated:
              size: 3
          dataPools:
            - failureDomain: host
              replicated:
                size: 3
              # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
              name: data0
          metadataServer:
            activeCount: 1
            activeStandby: true
            resources:
              limits:
                memory: "4Gi"
              requests:
                cpu: "1000m"
                memory: "1Gi"
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          isDefault: false
          name: ceph-filesystem
          # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
          pool: data0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: "Immediate"
          annotations: {}
          labels: {}
          mountOptions: []
          # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
          parameters:
            # The secrets contain Ceph admin credentials.
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
            # Specify the filesystem type of the volume. If not specified, csi-provisioner
            # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
            # in hyperconverged settings where the volume is mounted on the same node as the osds.
            csi.storage.k8s.io/fstype: ext4
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-blockpool
      isDefault: false
      deletionPolicy: Delete
    cephFileSystemVolumeSnapshotClass:
      enabled: true
      name: csi-ceph-filesystem
      isDefault: false
      deletionPolicy: Delete
    monitoring:
      enabled: true
    ingress:
      dashboard:
        annotations:
          kubernetes.io/ingress.class: "nginx"
          cert-manager.io/cluster-issuer: "cloudflare-issuer"
          nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
          nginx.ingress.kubernetes.io/server-snippet: |
            proxy_ssl_verify off;
        host:
          name: &host rook-ceph.invertedorigin.com
        tls:
        - hosts:
          - *host
          secretName: rook-ceph-tls-domain
