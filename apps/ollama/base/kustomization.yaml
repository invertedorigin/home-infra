apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- namespace.yaml

namespace: ollama

helmCharts:
- name: ollama
  repo: https://otwld.github.io/ollama-helm/
  version: v0.40.0
  releaseName: ollama
  valuesInLine:
    replicaCount: 3
    extraEnv:
      - name: HSA_OVERRIDE_GFX_VERSION
        value: "10.3.0"
      - name: HCC_AMDGPU_TARGETS
        value: "gfx1035"
    podLabels:
      app: ollama
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - ollama
          topologyKey: "kubernetes.io/hostname"
    ollama:
      models:
        - llama3:8b
      gpu:
        enabled: true
        type: "amd"
        number: 1
    ingress:
      enabled: true
      className: nginx
      annotations:
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/cluster-issuer: "cloudflare-issuer"
        nginx.ingress.kubernetes.io/proxy-body-size: "600m"
        nginx.org/client-max-body-size: "600m"
      hosts:
        - host: &host ollama-api.invertedorigin.com
          paths:
            - path: /
              pathType: Prefix
      tls:
        - hosts:
            - *host
          secretName: ollama-api-tls-domain

- name: open-webui
  repo: https://helm.openwebui.com/
  version: v3.0.5
  releaseName: open-webui
  valuesInLine:
    ollama:
      enabled: false
    pipelines:
      enabled: false
    ollamaUrls: ["http://ollama.ollama.svc.cluster.local:11434"]
    ingress:
      enabled: true
      class: "nginx"
      annotations:
        kubernetes.io/ingress.class: "nginx"
        cert-manager.io/cluster-issuer: "cloudflare-issuer"
        nginx.ingress.kubernetes.io/proxy-body-size: "600m"
        nginx.org/client-max-body-size: "600m"
      host: "ollama.invertedorigin.com"
      tls: true
